{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Telecommunications Fraud Detection Using MongoDB and Python**\n",
        "\n",
        "**Project Deliverable**\n",
        "\n",
        "● A GitHub repository with a python file (.py) with your solution.\n",
        "\n",
        "**Problem Statement**\n",
        "\n",
        "Telecommunications companies need to detect fraudulent activities such as unauthorized use of premium services or fake billing. Building a data pipeline with MongoDB and Python could help identify suspicious activity by extracting data from billing systems, call logs, and other sources, transforming the data to identify patterns or anomalies, and storing it in MongoDB for further\n",
        "analysis.\n",
        "\n",
        "**Background Information**\n",
        "\n",
        "Telecommunications companies generate a vast amount of data daily, which can be used to detect fraud. Fraudulent activity can lead to substantial financial losses and damage the\n",
        "company's reputation. With the help of data pipelines, companies can detect fraud before it escalates. Guidelines\n",
        "\n",
        "We will build a data pipeline with three main functions: extraction, transformation, and loading.\n",
        "\n",
        "The pipeline will extract data from CSV files, transform it to identify suspicious activities, and load it into MongoDB. We will use Python as the programming language, and Pymongo as the driver to interact with MongoDB.\n",
        "\n",
        "● Sample Datasets for Extraction: We will use sample call log data in CSV format as the dataset for extraction. The dataset will include fields such as call duration, call type, phone number, and time stamp.\n",
        "\n",
        "● Extraction Function: The extraction function will read data from CSV files and insert it into MongoDB. \n",
        "To optimize the data pipeline, we can use connection pooling to maintain open connections to the database. \n",
        "To secure the pipeline, we can use SSL encryption to encrypt the data transmitted between the client and server. We can also use logging to\n",
        "monitor the extraction process for errors and potential security breaches. \n",
        "Here are some guidelines for the extraction function:\n",
        "*   Use the pandas library to read the input CSV files.\n",
        "*   Clean and preprocess the data by removing duplicates, handling missing values, and converting data types.\n",
        "*   Use connection pooling to optimize performance.\n",
        "*   Use SSL encryption to secure the pipeline.\n",
        "*   Log errors and activities using the Python logging module.\n",
        "\n",
        "● Transformation Function: The transformation function will identify suspicious activities based on the data extracted from CSV files. \n",
        "We can use techniques such as aggregation and grouping to identify patterns and anomalies in the data. \n",
        "To optimize the data pipeline, we can use indexes to speed up the data retrieval process. \n",
        "To secure the pipeline, we can use data masking to protect sensitive data. Here are some guidelines for the transformation function:\n",
        "1.   Clean the data and handle missing values.\n",
        "2.   Group and aggregate the data by customer, location, time, and other relevant parameters.\n",
        "3.   Identify patterns in the data to detect suspicious activity, such as unauthorized use of premium services, fake billing, or international calls.\n",
        "4.   Use data compression techniques to optimize performance and reduce storage requirements.\n",
        "5.   Use the Python logging module to log errors and activities.\n",
        "\n",
        "● Loading Function: The loading function will insert the transformed data into MongoDB.\n",
        "We can use batch inserts to improve performance and reduce network traffic. To optimize the data pipeline, we can use sharding to distribute the data across multiple shards and balance the load. To secure the pipeline, we can use authentication and authorization to restrict access to the data. Here are some guidelines for the loading\n",
        "function:\n",
        "\n",
        "1.  Use the pymongo library to connect to the MongoDB instance.\n",
        "2.  Create a new MongoDB collection for each data source.\n",
        "3.  Create indexes on the collection to optimize queries and performance.\n",
        "4.  Use bulk inserts to optimize performance.\n",
        "5.  Use the write concern option to ensure that data is written to disk.\n",
        "6.  Use the Python logging module to log errors and activities.\n",
        "\n",
        "You can use the guiding file (https://bit.ly/3lV9fW3) as a starting point for your data pipeline.\n",
        "Sample Datasets for Data Extraction\n",
        "\n",
        "Here are some sample datasets (https://bit.ly/3YRn7z4) you can use for extraction:\n",
        "1. Call logs\n",
        "2. Billing systems"
      ],
      "metadata": {
        "id": "YVWYd8mqVxXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the necessary libraries**"
      ],
      "metadata": {
        "id": "AID178FBd1Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import pymongo\n",
        "import logging\n",
        "from pymongo import UpdateOne, DeleteOne\n",
        "from pymongo.errors import BulkWriteError"
      ],
      "metadata": {
        "id": "HVLowX16hKZl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extraction functions:-** I am defining two functions to extract \"call_logs.csv\" and \"billing_systems.csv\"bold text separately, the load the data to the respective DataFrames."
      ],
      "metadata": {
        "id": "fDsCbpJfd1nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraction functions\n",
        "def extract_call_logs():\n",
        "    \"\"\"Extract call logs from CSV file and convert call duration to minutes for easier analysis.\"\"\"\n",
        "    # Load call log data from CSV file\n",
        "    call_logs = pd.read_csv('call_logs.csv')\n",
        "\n",
        "    # Convert call duration to minutes for easier analysis\n",
        "    call_logs['duration_minutes'] = call_logs['call_duration'] / 60\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Call logs extraction completed.\")\n",
        "\n",
        "    return call_logs\n",
        "\n",
        "def extract_billing_systems():\n",
        "    \"\"\"Extract billing systems from CSV file.\"\"\"\n",
        "    # Load billing system data from CSV file\n",
        "    billing_systems = pd.read_csv('billing_systems.csv')\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Billing systems extraction completed.\")\n",
        "\n",
        "    return billing_systems"
      ],
      "metadata": {
        "id": "8zkJ6aDjhYkF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformation fucntions:-** After extracting the datasets, I am defining two transformation functions for the two DataFrames by dropping nulls and duploicates to have cleaner and transformed dataframes.\n"
      ],
      "metadata": {
        "id": "me9a8WNDd1hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation functions\n",
        "def transform_call_logs(call_logs):\n",
        "    \"\"\"Clean call logs data and transform it to a list of dictionaries.\"\"\"\n",
        "    # Data cleaning and handling missing values\n",
        "    transformed_data = call_logs.dropna()\n",
        "    transformed_data = transformed_data.drop_duplicates()\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Call logs transformation completed.\")\n",
        "    \n",
        "    transformed_data = transformed_data.to_dict('records')\n",
        "    \n",
        "    return transformed_data\n",
        "\n",
        "def transform_billing_systems(billing_systems):\n",
        "    \"\"\"Clean billing systems data and transform it to a list of dictionaries.\"\"\"\n",
        "    # Data cleaning and handling missing values\n",
        "    transformed_data = billing_systems.dropna()\n",
        "    transformed_data = transformed_data.drop_duplicates()\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Billing systems transformation completed.\")\n",
        "    \n",
        "    transformed_data = transformed_data.to_dict('records')\n",
        "    \n",
        "    return transformed_data\n"
      ],
      "metadata": {
        "id": "9JvSqt15hetM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading function:-** I am defining a loading function which first connects to the MongoDB, then insert the merged dataframe which is a combination of the transfromed_data for both call_logs and billing_systems DataFrames"
      ],
      "metadata": {
        "id": "Gya2ute8d1rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading function\n",
        "def load_data(merged_data):\n",
        "    \"\"\"Load merged data to MongoDB.\"\"\"\n",
        "    # Connect to MongoDB\n",
        "    client = pymongo.MongoClient(\"mongodb+srv://mongo:mongo@cluster0.yj2pr.mongodb.net/minPoolSize=5&maxPoolSize=10?retryWrites=true&w=majority\",ssl=True,tlsInsecure=True)\n",
        "    db = client[\"odenyire\"]\n",
        "    collection = db[\"odenyire\"]\n",
        "\n",
        "    # Create indexes on the collection and compress data using snappy algorithm\n",
        "    collection.create_index([('call_duration',pymongo.DESCENDING)],\n",
        "                            storageEngine={\n",
        "                                'wiredTiger': {\n",
        "                                    'configString': 'block_compressor=snappy'\n",
        "                                }\n",
        "                            }\n",
        "                           )\n",
        "\n",
        "    # Use bulk inserts to optimize performance\n",
        "    collection.insert_many(merged_data)\n",
        "\n",
        "    #Demonstrate the ability to execute mixed bulk write operations, will be combining one update and Delete operations\n",
        "    requests = [\n",
        "        UpdateOne({\"call_id\":1},{'$set':{'call_type':'Incoming'}}),\n",
        "        DeleteOne({'call_id':2})\n",
        "    ]\n",
        "    try:\n",
        "        collection.bulk_write(requests)\n",
        "    except BulkWriteError as bwe:\n",
        "        pprint(bwe.details)\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Data loading completed.\")\n"
      ],
      "metadata": {
        "id": "x6CSrNiYiISu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main function:-** Calling the main function"
      ],
      "metadata": {
        "id": "cLcW_hS9d1vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    call_logs = extract_call_logs()\n",
        "    billing_systems = extract_billing_systems()\n",
        "\n",
        "    transformed_call_logs = transform_call_logs(call_logs)\n",
        "    transformed_billing_systems = transform_billing_systems(billing_systems)\n",
        "\n",
        "    # Merge the two dataframes\n",
        "    merged_data = transformed_call_logs + transformed_billing_systems\n",
        "\n",
        "    load_data(merged_data)"
      ],
      "metadata": {
        "id": "B5MXYMFdifPE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete code for the pipeline:-** One line documentation and comments\n",
        "\n"
      ],
      "metadata": {
        "id": "TWeLFqKld2ML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import pymongo\n",
        "import logging\n",
        "from pymongo import UpdateOne, DeleteOne\n",
        "from pymongo.errors import BulkWriteError\n",
        "\n",
        "# Extraction functions\n",
        "def extract_call_logs():\n",
        "    # Load call log data from CSV file\n",
        "    call_logs = pd.read_csv('call_logs.csv')\n",
        "\n",
        "    # Convert call duration to minutes for easier analysis\n",
        "    call_logs['duration_minutes'] = call_logs['call_duration'] / 60\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Call logs extraction completed.\")\n",
        "\n",
        "    return call_logs\n",
        "\n",
        "def extract_billing_systems():\n",
        "    # Load billing system data from CSV file\n",
        "    billing_systems = pd.read_csv('billing_systems.csv')\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Billing systems extraction completed.\")\n",
        "\n",
        "    return billing_systems\n",
        "\n",
        "# Transformation functions\n",
        "def transform_call_logs(call_logs):\n",
        "    # Data cleaning and handling missing values\n",
        "    transformed_data = call_logs.dropna()\n",
        "    transformed_data = transformed_data.drop_duplicates()\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Call logs transformation completed.\")\n",
        "    \n",
        "    transformed_data = transformed_data.to_dict('records')\n",
        "    \n",
        "    return transformed_data\n",
        "\n",
        "def transform_billing_systems(billing_systems):\n",
        "    # Data cleaning and handling missing values\n",
        "    transformed_data = billing_systems.dropna()\n",
        "    transformed_data = transformed_data.drop_duplicates()\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Billing systems transformation completed.\")\n",
        "    \n",
        "    transformed_data = transformed_data.to_dict('records')\n",
        "    \n",
        "    return transformed_data\n",
        "\n",
        "# Loading function\n",
        "def load_data(merged_data):\n",
        "    # Connect to MongoDB\n",
        "    client = pymongo.MongoClient(\"mongodb+srv://mongo:mongo@cluster0.yj2pr.mongodb.net/minPoolSize=5&maxPoolSize=10?retryWrites=true&w=majority\",ssl=True,tlsInsecure=True)\n",
        "    db = client[\"odenyire\"]\n",
        "    collection = db[\"odenyire\"]\n",
        "\n",
        "    # Create indexes on the collection and compress data using snappy algorithm\n",
        "    collection.create_index([('call_duration',pymongo.DESCENDING)],\n",
        "                            storageEngine={\n",
        "                                'wiredTiger': {\n",
        "                                    'configString': 'block_compressor=snappy'\n",
        "                                }\n",
        "                            }\n",
        "                           )\n",
        "\n",
        "    # Use bulk inserts to optimize performance\n",
        "    collection.insert_many(merged_data)\n",
        "\n",
        "    #Demonstrate the ability to execute mixed bulk write operations, will be combining one update and Delete operations\n",
        "    requests = [\n",
        "        UpdateOne({\"call_id\":1},{'$set':{'call_type':'Incoming'}}),\n",
        "        DeleteOne({'call_id':2})\n",
        "    ]\n",
        "    try:\n",
        "        collection.bulk_write(requests)\n",
        "    except BulkWriteError as bwe:\n",
        "        pprint(bwe.details)\n",
        "\n",
        "    # Use Python logging module to log errors and activities\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Data loading completed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    call_logs = extract_call_logs()\n",
        "    billing_systems = extract_billing_systems()\n",
        "\n",
        "    transformed_call_logs = transform_call_logs(call_logs)\n",
        "    transformed_billing_systems = transform_billing_systems(billing_systems)\n",
        "\n",
        "    # Merge the two dataframes\n",
        "    merged_data = transformed_call_logs + transformed_billing_systems\n",
        "\n",
        "    load_data(merged_data)\n"
      ],
      "metadata": {
        "id": "XHkFniNYdeld"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete code for the pipeline:-** Documentation using docstring"
      ],
      "metadata": {
        "id": "SmErJ1NYme-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import pymongo\n",
        "import logging\n",
        "from pymongo import UpdateOne, DeleteOne\n",
        "from pymongo.errors import BulkWriteError\n",
        "\n",
        "def extract_call_logs():\n",
        "    \"\"\"Extract call logs from CSV file and convert call duration to minutes for easier analysis.\"\"\"\n",
        "    call_logs = pd.read_csv('call_logs.csv')\n",
        "    call_logs['duration_minutes'] = call_logs['call_duration'] / 60\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Call logs extraction completed.\")\n",
        "    return call_logs\n",
        "\n",
        "def extract_billing_systems():\n",
        "    \"\"\"Extract billing systems from CSV file.\"\"\"\n",
        "    billing_systems = pd.read_csv('billing_systems.csv')\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Billing systems extraction completed.\")\n",
        "    return billing_systems\n",
        "\n",
        "def transform_call_logs(call_logs):\n",
        "    \"\"\"Clean call logs data and transform it to a list of dictionaries.\"\"\"\n",
        "    transformed_data = call_logs.dropna().drop_duplicates()\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Call logs transformation completed.\")\n",
        "    transformed_data = transformed_data.to_dict('records')\n",
        "    return transformed_data\n",
        "\n",
        "def transform_billing_systems(billing_systems):\n",
        "    \"\"\"Clean billing systems data and transform it to a list of dictionaries.\"\"\"\n",
        "    transformed_data = billing_systems.dropna().drop_duplicates()\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Billing systems transformation completed.\")\n",
        "    transformed_data = transformed_data.to_dict('records')\n",
        "    return transformed_data\n",
        "\n",
        "def load_data(merged_data):\n",
        "    \"\"\"Load merged data to MongoDB.\"\"\"\n",
        "    client = pymongo.MongoClient(\"mongodb+srv://mongo:mongo@cluster0.yj2pr.mongodb.net/minPoolSize=5&maxPoolSize=10?retryWrites=true&w=majority\",ssl=True,tlsInsecure=True)\n",
        "    db = client[\"odenyire\"]\n",
        "    collection = db[\"odenyire\"]\n",
        "    collection.create_index([('call_duration',pymongo.DESCENDING)],\n",
        "                            storageEngine={\n",
        "                                'wiredTiger': {\n",
        "                                    'configString': 'block_compressor=snappy'\n",
        "                                }\n",
        "                            }\n",
        "                           )\n",
        "    collection.insert_many(merged_data)\n",
        "    requests = [\n",
        "        UpdateOne({\"call_id\":1},{'$set':{'call_type':'Incoming'}}),\n",
        "        DeleteOne({'call_id':2})\n",
        "    ]\n",
        "    try:\n",
        "        collection.bulk_write(requests)\n",
        "    except BulkWriteError as bwe:\n",
        "        pprint(bwe.details)\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info(\"Data loading completed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    call_logs = extract_call_logs()\n",
        "    billing_systems = extract_billing_systems()\n",
        "    transformed_call_logs = transform_call_logs(call_logs)\n",
        "    transformed_billing_systems = transform_billing_systems(billing_systems)\n",
        "    merged_data = transformed_call_logs + transformed_billing_systems\n",
        "    load_data(merged_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "UffO8xSwlCre"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}
# -*- coding: utf-8 -*-
"""Week 7 Telecommunications Fraud Detection Using MongoDB and Python - Emmanuel Anyira.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o2YeE1CGjPW4StIh39o-CSXfGym0u2ya

**Telecommunications Fraud Detection Using MongoDB and Python**

**Project Deliverable**

● A GitHub repository with a python file (.py) with your solution.

**Problem Statement**

Telecommunications companies need to detect fraudulent activities such as unauthorized use of premium services or fake billing. Building a data pipeline with MongoDB and Python could help identify suspicious activity by extracting data from billing systems, call logs, and other sources, transforming the data to identify patterns or anomalies, and storing it in MongoDB for further
analysis.

**Background Information**

Telecommunications companies generate a vast amount of data daily, which can be used to detect fraud. Fraudulent activity can lead to substantial financial losses and damage the
company's reputation. With the help of data pipelines, companies can detect fraud before it escalates. Guidelines

We will build a data pipeline with three main functions: extraction, transformation, and loading.

The pipeline will extract data from CSV files, transform it to identify suspicious activities, and load it into MongoDB. We will use Python as the programming language, and Pymongo as the driver to interact with MongoDB.

● Sample Datasets for Extraction: We will use sample call log data in CSV format as the dataset for extraction. The dataset will include fields such as call duration, call type, phone number, and time stamp.

● Extraction Function: The extraction function will read data from CSV files and insert it into MongoDB. 
To optimize the data pipeline, we can use connection pooling to maintain open connections to the database. 
To secure the pipeline, we can use SSL encryption to encrypt the data transmitted between the client and server. We can also use logging to
monitor the extraction process for errors and potential security breaches. 
Here are some guidelines for the extraction function:
*   Use the pandas library to read the input CSV files.
*   Clean and preprocess the data by removing duplicates, handling missing values, and converting data types.
*   Use connection pooling to optimize performance.
*   Use SSL encryption to secure the pipeline.
*   Log errors and activities using the Python logging module.

● Transformation Function: The transformation function will identify suspicious activities based on the data extracted from CSV files. 
We can use techniques such as aggregation and grouping to identify patterns and anomalies in the data. 
To optimize the data pipeline, we can use indexes to speed up the data retrieval process. 
To secure the pipeline, we can use data masking to protect sensitive data. Here are some guidelines for the transformation function:
1.   Clean the data and handle missing values.
2.   Group and aggregate the data by customer, location, time, and other relevant parameters.
3.   Identify patterns in the data to detect suspicious activity, such as unauthorized use of premium services, fake billing, or international calls.
4.   Use data compression techniques to optimize performance and reduce storage requirements.
5.   Use the Python logging module to log errors and activities.

● Loading Function: The loading function will insert the transformed data into MongoDB.
We can use batch inserts to improve performance and reduce network traffic. To optimize the data pipeline, we can use sharding to distribute the data across multiple shards and balance the load. To secure the pipeline, we can use authentication and authorization to restrict access to the data. Here are some guidelines for the loading
function:

1.  Use the pymongo library to connect to the MongoDB instance.
2.  Create a new MongoDB collection for each data source.
3.  Create indexes on the collection to optimize queries and performance.
4.  Use bulk inserts to optimize performance.
5.  Use the write concern option to ensure that data is written to disk.
6.  Use the Python logging module to log errors and activities.

You can use the guiding file (https://bit.ly/3lV9fW3) as a starting point for your data pipeline.
Sample Datasets for Data Extraction

Here are some sample datasets (https://bit.ly/3YRn7z4) you can use for extraction:
1. Call logs
2. Billing systems

**Importing the necessary libraries**
"""

from pprint import pprint
import pandas as pd
import pymongo
import logging
from pymongo import UpdateOne, DeleteOne
from pymongo.errors import BulkWriteError

"""**Extraction functions:-** I am defining two functions to extract "call_logs.csv" and "billing_systems.csv"bold text separately, the load the data to the respective DataFrames."""

# Extraction functions
def extract_call_logs():
    """Extract call logs from CSV file and convert call duration to minutes for easier analysis."""
    # Load call log data from CSV file
    call_logs = pd.read_csv('call_logs.csv')

    # Convert call duration to minutes for easier analysis
    call_logs['duration_minutes'] = call_logs['call_duration'] / 60

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Call logs extraction completed.")

    return call_logs

def extract_billing_systems():
    """Extract billing systems from CSV file."""
    # Load billing system data from CSV file
    billing_systems = pd.read_csv('billing_systems.csv')

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Billing systems extraction completed.")

    return billing_systems

"""**Transformation fucntions:-** After extracting the datasets, I am defining two transformation functions for the two DataFrames by dropping nulls and duploicates to have cleaner and transformed dataframes.

"""

# Transformation functions
def transform_call_logs(call_logs):
    """Clean call logs data and transform it to a list of dictionaries."""
    # Data cleaning and handling missing values
    transformed_data = call_logs.dropna()
    transformed_data = transformed_data.drop_duplicates()

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Call logs transformation completed.")
    
    transformed_data = transformed_data.to_dict('records')
    
    return transformed_data

def transform_billing_systems(billing_systems):
    """Clean billing systems data and transform it to a list of dictionaries."""
    # Data cleaning and handling missing values
    transformed_data = billing_systems.dropna()
    transformed_data = transformed_data.drop_duplicates()

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Billing systems transformation completed.")
    
    transformed_data = transformed_data.to_dict('records')
    
    return transformed_data

"""**Loading function:-** I am defining a loading function which first connects to the MongoDB, then insert the merged dataframe which is a combination of the transfromed_data for both call_logs and billing_systems DataFrames"""

# Loading function
def load_data(merged_data):
    """Load merged data to MongoDB."""
    # Connect to MongoDB
    client = pymongo.MongoClient("mongodb+srv://mongo:mongo@cluster0.yj2pr.mongodb.net/minPoolSize=5&maxPoolSize=10?retryWrites=true&w=majority",ssl=True,tlsInsecure=True)
    db = client["odenyire"]
    collection = db["odenyire"]

    # Create indexes on the collection and compress data using snappy algorithm
    collection.create_index([('call_duration',pymongo.DESCENDING)],
                            storageEngine={
                                'wiredTiger': {
                                    'configString': 'block_compressor=snappy'
                                }
                            }
                           )

    # Use bulk inserts to optimize performance
    collection.insert_many(merged_data)

    #Demonstrate the ability to execute mixed bulk write operations, will be combining one update and Delete operations
    requests = [
        UpdateOne({"call_id":1},{'$set':{'call_type':'Incoming'}}),
        DeleteOne({'call_id':2})
    ]
    try:
        collection.bulk_write(requests)
    except BulkWriteError as bwe:
        pprint(bwe.details)

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Data loading completed.")

"""**Main function:-** Calling the main function"""

if __name__ == '__main__':
    call_logs = extract_call_logs()
    billing_systems = extract_billing_systems()

    transformed_call_logs = transform_call_logs(call_logs)
    transformed_billing_systems = transform_billing_systems(billing_systems)

    # Merge the two dataframes
    merged_data = transformed_call_logs + transformed_billing_systems

    load_data(merged_data)

"""**Complete code for the pipeline:-** One line documentation and comments


"""

from pprint import pprint
import pandas as pd
import pymongo
import logging
from pymongo import UpdateOne, DeleteOne
from pymongo.errors import BulkWriteError

# Extraction functions
def extract_call_logs():
    # Load call log data from CSV file
    call_logs = pd.read_csv('call_logs.csv')

    # Convert call duration to minutes for easier analysis
    call_logs['duration_minutes'] = call_logs['call_duration'] / 60

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Call logs extraction completed.")

    return call_logs

def extract_billing_systems():
    # Load billing system data from CSV file
    billing_systems = pd.read_csv('billing_systems.csv')

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Billing systems extraction completed.")

    return billing_systems

# Transformation functions
def transform_call_logs(call_logs):
    # Data cleaning and handling missing values
    transformed_data = call_logs.dropna()
    transformed_data = transformed_data.drop_duplicates()

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Call logs transformation completed.")
    
    transformed_data = transformed_data.to_dict('records')
    
    return transformed_data

def transform_billing_systems(billing_systems):
    # Data cleaning and handling missing values
    transformed_data = billing_systems.dropna()
    transformed_data = transformed_data.drop_duplicates()

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Billing systems transformation completed.")
    
    transformed_data = transformed_data.to_dict('records')
    
    return transformed_data

# Loading function
def load_data(merged_data):
    # Connect to MongoDB
    client = pymongo.MongoClient("mongodb+srv://mongo:mongo@cluster0.yj2pr.mongodb.net/minPoolSize=5&maxPoolSize=10?retryWrites=true&w=majority",ssl=True,tlsInsecure=True)
    db = client["odenyire"]
    collection = db["odenyire"]

    # Create indexes on the collection and compress data using snappy algorithm
    collection.create_index([('call_duration',pymongo.DESCENDING)],
                            storageEngine={
                                'wiredTiger': {
                                    'configString': 'block_compressor=snappy'
                                }
                            }
                           )

    # Use bulk inserts to optimize performance
    collection.insert_many(merged_data)

    #Demonstrate the ability to execute mixed bulk write operations, will be combining one update and Delete operations
    requests = [
        UpdateOne({"call_id":1},{'$set':{'call_type':'Incoming'}}),
        DeleteOne({'call_id':2})
    ]
    try:
        collection.bulk_write(requests)
    except BulkWriteError as bwe:
        pprint(bwe.details)

    # Use Python logging module to log errors and activities
    logger = logging.getLogger(__name__)
    logger.info("Data loading completed.")

if __name__ == '__main__':
    call_logs = extract_call_logs()
    billing_systems = extract_billing_systems()

    transformed_call_logs = transform_call_logs(call_logs)
    transformed_billing_systems = transform_billing_systems(billing_systems)

    # Merge the two dataframes
    merged_data = transformed_call_logs + transformed_billing_systems

    load_data(merged_data)

"""**Complete code for the pipeline:-** Documentation using docstring"""

from pprint import pprint
import pandas as pd
import pymongo
import logging
from pymongo import UpdateOne, DeleteOne
from pymongo.errors import BulkWriteError

def extract_call_logs():
    """Extract call logs from CSV file and convert call duration to minutes for easier analysis."""
    call_logs = pd.read_csv('call_logs.csv')
    call_logs['duration_minutes'] = call_logs['call_duration'] / 60
    logger = logging.getLogger(__name__)
    logger.info("Call logs extraction completed.")
    return call_logs

def extract_billing_systems():
    """Extract billing systems from CSV file."""
    billing_systems = pd.read_csv('billing_systems.csv')
    logger = logging.getLogger(__name__)
    logger.info("Billing systems extraction completed.")
    return billing_systems

def transform_call_logs(call_logs):
    """Clean call logs data and transform it to a list of dictionaries."""
    transformed_data = call_logs.dropna().drop_duplicates()
    logger = logging.getLogger(__name__)
    logger.info("Call logs transformation completed.")
    transformed_data = transformed_data.to_dict('records')
    return transformed_data

def transform_billing_systems(billing_systems):
    """Clean billing systems data and transform it to a list of dictionaries."""
    transformed_data = billing_systems.dropna().drop_duplicates()
    logger = logging.getLogger(__name__)
    logger.info("Billing systems transformation completed.")
    transformed_data = transformed_data.to_dict('records')
    return transformed_data

def load_data(merged_data):
    """Load merged data to MongoDB."""
    client = pymongo.MongoClient("mongodb+srv://mongo:mongo@cluster0.yj2pr.mongodb.net/minPoolSize=5&maxPoolSize=10?retryWrites=true&w=majority",ssl=True,tlsInsecure=True)
    db = client["odenyire"]
    collection = db["odenyire"]
    collection.create_index([('call_duration',pymongo.DESCENDING)],
                            storageEngine={
                                'wiredTiger': {
                                    'configString': 'block_compressor=snappy'
                                }
                            }
                           )
    collection.insert_many(merged_data)
    requests = [
        UpdateOne({"call_id":1},{'$set':{'call_type':'Incoming'}}),
        DeleteOne({'call_id':2})
    ]
    try:
        collection.bulk_write(requests)
    except BulkWriteError as bwe:
        pprint(bwe.details)
    logger = logging.getLogger(__name__)
    logger.info("Data loading completed.")

if __name__ == '__main__':
    call_logs = extract_call_logs()
    billing_systems = extract_billing_systems()
    transformed_call_logs = transform_call_logs(call_logs)
    transformed_billing_systems = transform_billing_systems(billing_systems)
    merged_data = transformed_call_logs + transformed_billing_systems
    load_data(merged_data)